{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYARLpkfBjF"
      },
      "source": [
        "# Project  TDDE09\n",
        "## Hugo Bjork || Jakob Berggren || Martin Forsberg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwiPZum4uix1"
      },
      "source": [
        "For this project we will need to run it on the GPU to optimize speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "1LGzWEOnVyAr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svv-2WiVVyAs",
        "outputId": "7444d868-dad2-4f93-9221-1538dfb3d491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# If on mac:\n",
        "# setting up mps as device. \n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "IjZFoilkuix1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819bbde4-1a5e-46cb-e157-7cceeb5e8658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#!pip install nltk\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXTrlu1-fIri"
      },
      "source": [
        "## The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYP7pMf1uix4"
      },
      "source": [
        "The data used fo this project is movie reviews from Imdb. The data set consists of 50 000 reviews labeled positive or negative. Our first course of action was to slim the data set down to only the revies with less than 128 words in them. This is done in order to be able to train the model within a resonable timeframe and avoid needing to chop up the reviews into chunks due to BERTs max length of 512.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnjtR3s3uix4",
        "outputId": "15dc2cc8-361c-402e-a73a-1ac1144ecb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n",
            "(13274, 2)\n",
            "51.94364923911405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-70d2e23cd478>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['sentiment'] = df['sentiment'].map(label_map)\n"
          ]
        }
      ],
      "source": [
        "#!pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('imdb.csv')\n",
        "print(df.shape)\n",
        "df = df[df['review'].str.split().apply(len) <= 128]\n",
        "print(df.shape)\n",
        "label_map = {'positive': 1, 'negative': 0}\n",
        "df['sentiment'] = df['sentiment'].map(label_map)\n",
        "print((df[df['sentiment']==1].count()[0]/len(df))*100)\n",
        "\n",
        "train = df[0:int(len(df)*0.075)]\n",
        "test = df[int(len(df)*0.075):int(len(df)*0.1)]\n",
        "vadScore = [sid.polarity_scores(a) for a in train.review]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMyfvzLuix5"
      },
      "source": [
        "The data is loaded and preproccessed into a smaller set of max review length of 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "ufyC5VtJuix6"
      },
      "outputs": [],
      "source": [
        "def preVader(data):\n",
        "    vadScore = [sid.polarity_scores(a) for a in data.review]\n",
        "    columns = list(vadScore[0].keys())\n",
        "    tens = torch.empty(len(vadScore), len(columns)).to(device)\n",
        "\n",
        "    # All reviews for traning where columns is neg&neu&pos&comp\n",
        "    for i, score in enumerate(vadScore):\n",
        "        for j, key in enumerate(columns):\n",
        "            tens[i, j] = score[key]\n",
        "    return tens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to1hvoj8uix6",
        "outputId": "5b06d1a6-e2bd-4bbb-eea4-293a56b27a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0170, 0.7580, 0.2250],\n",
            "        [0.0940, 0.5310, 0.3750],\n",
            "        [0.0840, 0.6960, 0.2210],\n",
            "        [0.0860, 0.7950, 0.1200]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "tens = preVader(train)\n",
        "print(tens[:4,:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAo0Vv_uix6"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC8OwgS8uix6"
      },
      "source": [
        "Our first task is to create our baseline by fine tuning the BERT model to our IMDB data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IuPZ7c1uix7"
      },
      "source": [
        "We need two classes from the Transformers library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xZ3x_MQvYuK",
        "outputId": "072b08a0-e362-44e1-8a2c-da2292b4cf9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "#Instantiating both classes with the pre-trained bert-base-uncased model.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbJDQlxmuix7"
      },
      "source": [
        "In the tensorize function the data is preproccessed to fit the bert modle requirements by translating the reviews to token ids, masking the padding tokens and finaly a tensor with the labels correspinding to each reviews. These are returned by a TensorDataset so it can easily be split by a dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "9DCuYESsuix8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "MAX_LENGTH = 130\n",
        "\n",
        "def tensorize(reviews):\n",
        "    input_ids = []\n",
        "    labels = []\n",
        "    attention_masks = []\n",
        "    for index, rev in reviews.iterrows():\n",
        "        encoded = tokenizer.encode_plus(\n",
        "                    rev[0].split(), \n",
        "                    add_special_tokens=True, \n",
        "                    max_length=MAX_LENGTH,\n",
        "                    padding='max_length',\n",
        "                    return_attention_mask=True,\n",
        "                    return_tensors='pt',\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "        labels.append(rev[1])\n",
        "    return TensorDataset(torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0), torch.tensor(labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlHNdHPuix8"
      },
      "source": [
        "Below we will make a dataloader class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "8uXTVDoHuix8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset = tensorize(train.iloc[:11,:]) \n",
        "datalord = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "FvvIUO4iuix8"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn\n",
        "#!pip install tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import *\n",
        "\n",
        "def train_bert(n_epochs=1, batch_size=32):\n",
        "    train = df[0:int(len(df)*0.75)]\n",
        "    test = df[int(len(df)*0.75):]\n",
        "    \n",
        "    train = tensorize(train)\n",
        "    test = tensorize(test)\n",
        "        \n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = False).to(device)\n",
        "    \n",
        "    # Initialize the optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "    \n",
        "    # set model in training mode\n",
        "    model.train()\n",
        "    for i in range(n_epochs):\n",
        "        with tqdm(total=len(list(train))) as pbar:\n",
        "            for index, batch in enumerate(DataLoader(train, batch_size, shuffle=False)): \n",
        "\n",
        "               # Reset the accumulated gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                b_ids = batch[0].to(device)\n",
        "                b_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                outputs = model(input_ids=b_ids, attention_mask=b_mask,\n",
        "                        labels=b_labels)\n",
        "                \n",
        "                # Backward pass; propagates the loss and computes the gradients\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the parameters of the model\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Update diagnostics\n",
        "                pbar.set_description(f'Epoch {i + 1}')\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                pbar.update(batch_size)\n",
        "                \n",
        "    model.eval()  # Sets the model to evaluation mode\n",
        "    with torch.no_grad():  # Blocks the accumulation of gradients\n",
        "        y,p = [], [] # init list of actuals and predictions\n",
        "        with tqdm(total=len(list(test))) as pbar:\n",
        "            for index, batch in enumerate(DataLoader(test, batch_size, shuffle=False)): \n",
        "                \n",
        "                b_ids = batch[0].to(device)\n",
        "                b_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                outputs = model(input_ids=b_ids, attention_mask=b_mask,\n",
        "                                labels=b_labels)\n",
        "                \n",
        "                loss = outputs.loss     \n",
        "                pred = torch.argmax(outputs[1], dim=1)\n",
        "                                \n",
        "                y.extend(b_labels.cpu().detach().numpy())\n",
        "                p.extend(pred.cpu().detach().numpy())\n",
        "                \n",
        "                # Update diagnostics\n",
        "                pbar.set_description(f'test')\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "            # using sklearn to compute performance metrics\n",
        "            cm = confusion_matrix(y, p)\n",
        "            acc = accuracy_score(y, p)\n",
        "            prec = precision_score(y, p)\n",
        "            rec = recall_score(y, p)\n",
        "            f1 = f1_score(y, p)\n",
        "            \n",
        "            # update status bar with metrics\n",
        "            pbar.set_postfix(accuracy=float(acc)*100,\n",
        "                              precision=float(prec)*100,\n",
        "                              recall=float(rec)*100,\n",
        "                              f1_score=float(f1)*100)\n",
        "            print(f\"\\n{cm}, CONFUSION MATRIX\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH9OCuh9uix9",
        "outputId": "e8ad973b-9bee-4891-b68b-c61b77da6251",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: : 9984it [03:28, 47.87it/s, loss=0.677]\n",
            "test: : 3328it [00:23, 143.77it/s, accuracy=87.6, f1_score=88.5, precision=84.6, recall=92.7]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[1324  289]\n",
            " [ 124 1582]], CONFUSION MATRIX\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bert=train_bert(n_epochs=1, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGv5EFsTVyAx"
      },
      "source": [
        "## Method 1 - MLP architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SwkUo9dVyAx"
      },
      "source": [
        "Next, we will build the architecture for a multi layer perceptron, which will be used to align our data in the same vector space. Later, the last linear layer in this model will be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "bPwQouzSuix7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq = nn.Sequential(\n",
        "                nn.LazyLinear(hidden_dim),\n",
        "                torch.nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, output_dim),\n",
        "       )\n",
        "    def forward(self, bert, vader):\n",
        "        x = torch.cat((bert, vader), dim=1)\n",
        "        x =  self.seq(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "pCJ_D43Cuix9"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def train_mlp(bert, n_epochs=1, batch_size=32, learning_rate=1e-5):\n",
        "    train = df[0:int(len(df)*0.75)]\n",
        "    test = df[len(train):]\n",
        "\n",
        "    vader_tr = preVader(train).to(device)\n",
        "    vad_pos = vader_tr[:, 2]\n",
        "    vad_neg = vader_tr[:, 0]\n",
        "    vader_tr = F.softmax(torch.column_stack((vad_neg, vad_pos)))\n",
        "\n",
        "    vader_te = preVader(test).to(device)\n",
        "    vad_pos = vader_te[:, 2]\n",
        "    vad_neg = vader_te[:, 0]\n",
        "    vader_te = F.softmax(torch.column_stack((vad_neg, vad_pos)))\n",
        "\n",
        "    train = tensorize(train)\n",
        "    test = tensorize(test)\n",
        "    \n",
        "    # init multi layer perceptron\n",
        "    mlp = MultilayerPerceptron(250, 2).to(device)\n",
        "        \n",
        "    # Initialize the optimizer. Here we use Adam rather than plain SGD\n",
        "    optimizer = optim.Adam(mlp.parameters(), lr=learning_rate)\n",
        "    mlp.train()\n",
        "    \n",
        "    for i in range(n_epochs):\n",
        "        with tqdm(total=len(list(train))) as pbar:\n",
        "            for index, batch in enumerate(DataLoader(train, batch_size, shuffle=False)): \n",
        "\n",
        "               # Reset the accumulated gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                b_ids = batch[0].to(device)\n",
        "                b_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                outputs = bert(input_ids=b_ids, attention_mask=b_mask,\n",
        "                        labels=b_labels)\n",
        "\n",
        "                # combine features of bert and vader using the mlp\n",
        "                mlp_out = mlp.forward(outputs.logits, vader_tr[index*batch_size:index*batch_size+outputs.logits.size(0),:3]) \n",
        "             \n",
        "                # Backward pass; propagates the loss and computes the gradients\n",
        "                loss = F.cross_entropy(mlp_out, b_labels)\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the parameters of the model\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update diagnostics\n",
        "                pbar.set_description(f'Epoch {i + 1}')\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                pbar.update(batch_size)\n",
        "                \n",
        "    model.eval()  # Sets the model to evaluation mode\n",
        "    with torch.no_grad():  # Blocks the accumulation of gradients\n",
        "        y,p = [], [] # init list of actuals and predictions\n",
        "        with tqdm(total=len(list(test))) as pbar:\n",
        "            for index, batch in enumerate(DataLoader(test, batch_size, shuffle=False)): \n",
        "              \n",
        "                b_ids = batch[0].to(device)\n",
        "                b_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "                \n",
        "                outputs = bert(input_ids=b_ids, attention_mask=b_mask,\n",
        "                                labels=b_labels)\n",
        "                \n",
        "                mlp_out = mlp.forward(outputs.logits, vader_te[index*batch_size:index*batch_size+outputs.logits.size(0),:3]) \n",
        "                \n",
        "                loss = F.cross_entropy(mlp_out, b_labels)\n",
        "                pred = torch.argmax(mlp_out, dim=1)\n",
        "                \n",
        "                y.extend(b_labels.cpu().detach().numpy())\n",
        "                p.extend(pred.cpu().detach().numpy())\n",
        "\n",
        "                # Update diagnostics\n",
        "                pbar.set_description(f'test')\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "            # using sklearn to compute performance metrics\n",
        "            cm = confusion_matrix(y, p)\n",
        "            acc = accuracy_score(y, p)\n",
        "            prec = precision_score(y, p)\n",
        "            rec = recall_score(y, p)\n",
        "            f1 = f1_score(y, p)\n",
        "            \n",
        "            # update status bar with metrics\n",
        "            pbar.set_postfix(accuracy=float(acc)*100,\n",
        "                              precision=float(prec)*100,\n",
        "                              recall=float(rec)*100,\n",
        "                              f1_score=float(f1)*100)\n",
        "            print(f\"\\n{cm}, CONFUSION MATRIX\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922Umg2cuix9",
        "outputId": "1e2a2e0b-a888-4180-df18-2256cf94018b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-98-f97e1d226e80>:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  vader_tr = F.softmax(torch.column_stack((vad_neg, vad_pos)))\n",
            "<ipython-input-98-f97e1d226e80>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  vader_te = F.softmax(torch.column_stack((vad_neg, vad_pos)))\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "Epoch 1: : 9984it [03:14, 51.25it/s, loss=0.394]                        \n",
            "test: : 3328it [00:23, 144.18it/s, accuracy=88.4, f1_score=88.9, precision=87.7, recall=90.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[1397  216]\n",
            " [ 168 1538]], CONFUSION MATRIX\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "mlp_model = train_mlp(bert, n_epochs=1, batch_size=32, learning_rate=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoQemrQ4uix9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save the model to a file\n",
        "# Denna kodsnutt laddar ner modellen till report, tryck refresh i colab så ser man den, sen kan man importa till django\n",
        "with open('./model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}