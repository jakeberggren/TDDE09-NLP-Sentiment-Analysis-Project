{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psYARLpkfBjF"
   },
   "source": [
    "# Project  Baseline TDDE09\n",
    "## Hugo Bjork || Jakob Berggren || Martin Forsberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHFJ9olsfuOO"
   },
   "source": [
    "For this project we will need to run it on the GPU to optimize speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NWurVptwtu8M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgzkYKwoIHAN"
   },
   "outputs": [],
   "source": [
    "# if cuda memory is full, run this\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXTrlu1-fIri"
   },
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyXxpe4ifuOQ"
   },
   "source": [
    "The data used for this project is movie reviews from Imdb. The data set consists of 50 000 reviews labeled positive or negative. We have chosen to slim down the dataset to only include reviews with 256 words or less in them. This is done in order to train the model within a reasonable time frame since and avoid the need to chop up reviews into chunks due to BERTs max length of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iY5KAqe7fuOQ",
    "outputId": "15a0c547-6299-426e-f1d9-8f1609bb3fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "(35711, 2)\n",
      "49.83618492901347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-bc8161bde7c6>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment'].map(label_map)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('imdb.csv')\n",
    "print(df.shape)\n",
    "df = df[df['review'].str.split().apply(len) <= 256]\n",
    "print(df.shape)\n",
    "label_map = {'positive': 1, 'negative': 0}\n",
    "df['sentiment'] = df['sentiment'].map(label_map)\n",
    "print((df[df['sentiment']==1].count()[0]/len(df))*100)\n",
    "\n",
    "train = df[0:int(len(df)*0.75)]\n",
    "test = df[int(len(df)*0.75):int(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "herfVJ-9fuOR"
   },
   "source": [
    "The data is loaded and preproccessed into a smaller set of max review length of 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCnoLNC9fuOR"
   },
   "source": [
    "Here is an example from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "H4h9Ep3StosJ",
    "outputId": "b84b54ac-2e5f-46ae-9925-c429984f940e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-366f918f-a404-4a0b-8c14-1525e17d4495\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-366f918f-a404-4a0b-8c14-1525e17d4495')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-366f918f-a404-4a0b-8c14-1525e17d4495 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-366f918f-a404-4a0b-8c14-1525e17d4495');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "1  A wonderful little production. <br /><br />The...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aswgLhgAfuOR"
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2EgfzcjfuOR"
   },
   "source": [
    "Our first task is to create our baseline by fine tuning the BERT model to our IMDB data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHbIblbqfuOS"
   },
   "source": [
    "We need two classes from the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xZ3x_MQvYuK",
    "outputId": "d4a90232-fadb-44f6-b33b-4b9d7ba77d2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "#Instantiating both classes with the pre-trained bert-base-uncased model.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2eekitCfuOS"
   },
   "source": [
    "In the tensorize function the data is preproccessed to fit the bert modle requirements by translating the reviews to token ids, masking the padding tokens and finaly a tensor with the labels correspinding to each reviews. These are returned by a TensorDataset so it can easily be split by a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7rORgRonfuOS"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def tensorize(reviews):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_masks = []\n",
    "    for index, rev in reviews.iterrows():\n",
    "        encoded = tokenizer.encode_plus(\n",
    "                    rev[0].split(), \n",
    "                    add_special_tokens=True, \n",
    "                    max_length=258,\n",
    "                    padding='max_length',\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt', \n",
    "       )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        labels.append(rev[1])\n",
    "    return TensorDataset(torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0), torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iS2BLQnfuOT"
   },
   "source": [
    "A dataloader class is created, which pre computes inputs, masks and labels. We chose a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTOLJsFkfuOT",
    "outputId": "bafa3e50-5475-4b1f-f47c-ef82f248b447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 0\n",
      "[tensor([[ 101,  100, 2265,  ...,    0,    0,    0],\n",
      "        [ 101,  100, 2469,  ...,    0,    0,    0],\n",
      "        [ 101,  100, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  100, 2245,  ...,    0,    0,    0],\n",
      "        [ 101,  100, 2011,  ...,    0,    0,    0],\n",
      "        [ 101,  100,  100,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset= tensorize(train.iloc[:11,:]) \n",
    "datalord = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for i,data in enumerate(datalord):\n",
    "    print('BATCH', i)\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "67eegBnQfuOT"
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def train_bert(n_epochs=1, batch_size=32):\n",
    "    train = df[0:int(len(df)*0.75)]\n",
    "    test = df[int(len(df)*0.75):int(len(df))]\n",
    "\n",
    "    train = tensorize(train)\n",
    "    test= tensorize(test)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels = 2, output_attentions = False,output_hidden_states = False).to(device)\n",
    "\n",
    "    true = 0\n",
    "    counter=0\n",
    "    size_last_batch=(len(test) % batch_size)\n",
    "\n",
    "    # Initialize the optimizer. Here we use Adam rather than plain SGD\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(n_epochs):\n",
    "        with tqdm(total=len(list(train))) as pbar:\n",
    "            for index, batch in enumerate(DataLoader(train, batch_size, shuffle=True)):\n",
    "\n",
    "               # Reset the accumulated gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                b_ids = batch[0].to(device)\n",
    "                b_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "\n",
    "                outputs = model(input_ids=b_ids, attention_mask=b_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "                # Backward pass; propagates the loss and computes the gradients\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters of the model\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update diagnostics\n",
    "                pbar.set_description(f'Epoch {i + 1}')\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "    with torch.no_grad():  # Blocks the accumulation of gradients\n",
    "        TP_FP,TP,FN, y,p=0,0,0,[],[]\n",
    "        with tqdm(total=len(list(test))) as pbar:\n",
    "          for index, batch in enumerate(DataLoader(test, batch_size, shuffle=True)): \n",
    "              counter +=1 \n",
    "              b_ids = batch[0].to(device)\n",
    "              b_mask = batch[1].to(device)\n",
    "              b_labels = batch[2].to(device)\n",
    "              outputs = model(input_ids=b_ids, attention_mask=b_mask,\n",
    "                              labels=b_labels)\n",
    "              loss = outputs.loss\n",
    "              pred = torch.argmax(outputs[1], dim=1 )\n",
    "              TP_FP +=sum(pred==1)\n",
    "              TP+=torch.sum((pred==1) & (b_labels==1))\n",
    "              FN +=torch.sum((pred==0) & (b_labels==1))\n",
    "              true += sum(pred == b_labels)\n",
    "              y.extend(b_labels.cpu().detach().numpy())\n",
    "              p.extend(pred.cpu().detach().numpy())\n",
    "\n",
    "              # Update diagnostics\n",
    "              pbar.set_description(f'training')\n",
    "              pbar.update(batch_size)\n",
    "\n",
    "          cm = confusion_matrix(y, p)\n",
    "          Precision =TP/TP_FP\n",
    "          Recall = TP/(TP + FN)\n",
    "          F1_score=(2 * Precision * Recall)/(Precision + Recall)\n",
    "          acc = true/(((counter-1)*batch_size)+size_last_batch)\n",
    "          pbar.set_postfix(accuracy=float(acc)*100,\n",
    "                           precision=float(Precision)*100,\n",
    "                           recall=float(Recall)*100,\n",
    "                           f1_score=float(F1_score)*100)\n",
    "          print(f\"\\n{cm}, CONFUSION MATRIX\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngkZiahHfuOU",
    "outputId": "dfb2b428-1d05-4cda-e80a-8b144adac068"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: : 26784it [19:14, 23.20it/s, loss=0.248]\n",
      "training: 100%|██████████| 8928/8928 [02:24<00:00, 61.90it/s, accuracy=89.7, f1_score=89.5, precision=88, recall=91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[3941  551]\n",
      " [ 400 4036]], CONFUSION MATRIX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model=train_bert(n_epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "27Ay9Mii9-ep"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the model to a file\n",
    "with open('./model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
